{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray \n",
    "import gym \n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestEnv(gym.Env):\n",
    "    def __init__(self, env_config):\n",
    "        self.action_space = gym.spaces.Discrete(2)\n",
    "        self.observation_space = gym.spaces.Discrete(100)\n",
    "        \n",
    "    def reset(self):\n",
    "        self.state = np.random.randint(100)        \n",
    "        return self.state\n",
    "    \n",
    "    def step(self, action):\n",
    "        self.state += 1 if action ==1 else -1\n",
    "        if self.state <=0 or self.state >=99:\n",
    "            done =True \n",
    "        else:\n",
    "            done=False \n",
    "            \n",
    "        return self.state, -100, done, {}\n",
    "    \n",
    "class RewardWrapper(gym.RewardWrapper):\n",
    "    def __init__(self, env, config):\n",
    "        super().__init__(env)\n",
    "        \n",
    "    def reset(self):\n",
    "        obs = self.env.reset()\n",
    "        self.timestep = 0\n",
    "        return obs \n",
    "    \n",
    "    def step(self, action):\n",
    "        state, reward, done, info = self.env.step(action)\n",
    "        \n",
    "        if self.timestep >100:\n",
    "            done = True\n",
    "        if done:\n",
    "            reward = 100 - self.timestep\n",
    "        else:\n",
    "            reward = 0\n",
    "        return state, reward, done, info\n",
    "\n",
    "\n",
    "def deeping_env_creator(env_config):\n",
    "    env = TestEnv(env_config)\n",
    "    env = RewardWrapper(env, env_config)\n",
    "    return env \n",
    "\n",
    "from ray.tune import register_env\n",
    "\n",
    "register_env(\"test_env\", deeping_env_creator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Current time: 2022-02-10 14:40:10 (running for 00:00:00.11)\n",
      "Memory usage on this node: 14.4/16.0 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 0/8 CPUs, 0/0 GPUs, 0.0/8.44 GiB heap, 0.0/4.22 GiB objects\n",
      "Result logdir: /Users/bumjin/ray_results/DQN\n",
      "Number of trials: 1/1 (1 PENDING)\n",
      "+--------------------------+----------+-------+\n",
      "| Trial name               | status   | loc   |\n",
      "|--------------------------+----------+-------|\n",
      "| DQN_test_env_e9055_00000 | PENDING  |       |\n",
      "+--------------------------+----------+-------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(DQNTrainer pid=6613)\u001b[0m 2022-02-10 14:40:11,176\tINFO simple_q.py:153 -- In multi-agent mode, policies will be optimized sequentially by the multi-GPU optimizer. Consider setting `simple_optimizer=True` if this doesn't work for you.\n",
      "\u001b[2m\u001b[36m(DQNTrainer pid=6613)\u001b[0m 2022-02-10 14:40:11,177\tINFO trainer.py:790 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(DQNTrainer pid=6613)\u001b[0m 2022-02-10 14:40:11,198\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(DQNTrainer pid=6613)\u001b[0m 2022-02-10 14:40:11,199\tWARNING util.py:55 -- Install gputil for GPU system monitoring.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for DQN_test_env_e9055_00000:\n",
      "  agent_timesteps_total: 1000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-02-10_14-40-12\n",
      "  done: false\n",
      "  episode_len_mean: 997.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 100.0\n",
      "  episode_reward_mean: 100.0\n",
      "  episode_reward_min: 100.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 1\n",
      "  experiment_id: 797275ba0b6b4e2cb6999971431c8815\n",
      "  hostname: bagbeomjins-MacBook-Pro.local\n",
      "  info:\n",
      "    last_target_update_ts: 1000\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_lr: 0.0005\n",
      "          grad_gnorm: 0.10335072875022888\n",
      "          max_q: 0.07963812351226807\n",
      "          mean_q: -0.1031285896897316\n",
      "          min_q: -0.4210588335990906\n",
      "        mean_td_error: -0.01788289099931717\n",
      "        model: {}\n",
      "        td_error:\n",
      "        - 0.11982420086860657\n",
      "        - -0.004023246467113495\n",
      "        - -0.12588927149772644\n",
      "        - -0.054803140461444855\n",
      "        - -0.017689034342765808\n",
      "        - 0.1207609623670578\n",
      "        - 0.04553477466106415\n",
      "        - 0.025608837604522705\n",
      "        - 0.0256088525056839\n",
      "        - -0.03558240830898285\n",
      "        - 0.15750180184841156\n",
      "        - 0.08008764684200287\n",
      "        - -0.28922396898269653\n",
      "        - -0.01020844280719757\n",
      "        - 0.08008764684200287\n",
      "        - 0.08008764684200287\n",
      "        - -0.07857654988765717\n",
      "        - 0.1207609623670578\n",
      "        - -0.057230520993471146\n",
      "        - 0.1178581565618515\n",
      "        - -0.004023276269435883\n",
      "        - 0.08008764684200287\n",
      "        - -0.3212229311466217\n",
      "        - -0.13591818511486053\n",
      "        - -0.14556783437728882\n",
      "        - -0.3144412040710449\n",
      "        - 0.11982420086860657\n",
      "        - -0.03355681896209717\n",
      "        - 0.08853171765804291\n",
      "        - 0.17531120777130127\n",
      "        - -0.15930867195129395\n",
      "        - -0.22246325016021729\n",
      "    num_agent_steps_sampled: 1000\n",
      "    num_agent_steps_trained: 32\n",
      "    num_steps_sampled: 1000\n",
      "    num_steps_trained: 32\n",
      "    num_steps_trained_this_iter: 32\n",
      "    num_target_updates: 1\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 21.2\n",
      "    ram_util_percent: 90.2\n",
      "  pid: 6613\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.025778026371211796\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.019466007625187312\n",
      "    mean_inference_ms: 0.5433225965166426\n",
      "    mean_raw_obs_processing_ms: 0.0809834791825606\n",
      "  time_since_restore: 0.8076438903808594\n",
      "  time_this_iter_s: 0.8076438903808594\n",
      "  time_total_s: 0.8076438903808594\n",
      "  timers:\n",
      "    learn_throughput: 1411.986\n",
      "    learn_time_ms: 22.663\n",
      "    load_throughput: 187193.484\n",
      "    load_time_ms: 0.171\n",
      "  timestamp: 1644471612\n",
      "  timesteps_since_restore: 32\n",
      "  timesteps_this_iter: 32\n",
      "  timesteps_total: 1000\n",
      "  training_iteration: 1\n",
      "  trial_id: e9055_00000\n",
      "  \n",
      "== Status ==\n",
      "Current time: 2022-02-10 14:40:15 (running for 00:00:05.13)\n",
      "Memory usage on this node: 14.4/16.0 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 1.0/8 CPUs, 0/0 GPUs, 0.0/8.44 GiB heap, 0.0/4.22 GiB objects\n",
      "Result logdir: /Users/bumjin/ray_results/DQN\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+--------------------------+----------+----------------+--------+------------------+------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name               | status   | loc            |   iter |   total time (s) |   ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|--------------------------+----------+----------------+--------+------------------+------+----------+----------------------+----------------------+--------------------|\n",
      "| DQN_test_env_e9055_00000 | RUNNING  | 127.0.0.1:6613 |      1 |         0.807644 | 1000 |      100 |                  100 |                  100 |                997 |\n",
      "+--------------------------+----------+----------------+--------+------------------+------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for DQN_test_env_e9055_00000:\n",
      "  agent_timesteps_total: 2000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-02-10_14-40-15\n",
      "  done: true\n",
      "  episode_len_mean: 997.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 100.0\n",
      "  episode_reward_mean: 100.0\n",
      "  episode_reward_min: 100.0\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 1\n",
      "  experiment_id: 797275ba0b6b4e2cb6999971431c8815\n",
      "  hostname: bagbeomjins-MacBook-Pro.local\n",
      "  info:\n",
      "    last_target_update_ts: 1504\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_lr: 0.0005\n",
      "          grad_gnorm: 0.9581133127212524\n",
      "          max_q: 99.81559753417969\n",
      "          mean_q: 3.2402069568634033\n",
      "          min_q: -0.9638577699661255\n",
      "        mean_td_error: 0.16137143969535828\n",
      "        model: {}\n",
      "        td_error:\n",
      "        - -0.44880861043930054\n",
      "        - 0.37850987911224365\n",
      "        - 0.322170615196228\n",
      "        - 0.23108229041099548\n",
      "        - 5.5842180252075195\n",
      "        - -0.07109847664833069\n",
      "        - -0.5789779424667358\n",
      "        - 0.028699103742837906\n",
      "        - -0.1844024658203125\n",
      "        - -1.0294657945632935\n",
      "        - 1.2675235271453857\n",
      "        - 0.37850987911224365\n",
      "        - -0.5821936130523682\n",
      "        - 0.08466671407222748\n",
      "        - 0.004675723612308502\n",
      "        - -0.032206445932388306\n",
      "        - -0.07478117942810059\n",
      "        - 0.028699103742837906\n",
      "        - -0.5821936130523682\n",
      "        - 0.322170615196228\n",
      "        - 0.004675723612308502\n",
      "        - 0.004675701260566711\n",
      "        - 0.04174182564020157\n",
      "        - 1.2675235271453857\n",
      "        - 0.06914719939231873\n",
      "        - -1.0294657945632935\n",
      "        - -0.9753924608230591\n",
      "        - 0.6216068267822266\n",
      "        - -0.2763916552066803\n",
      "        - 0.37850987911224365\n",
      "        - 0.14262478053569794\n",
      "        - -0.13216713070869446\n",
      "    num_agent_steps_sampled: 2000\n",
      "    num_agent_steps_trained: 8032\n",
      "    num_steps_sampled: 2000\n",
      "    num_steps_trained: 8032\n",
      "    num_steps_trained_this_iter: 32\n",
      "    num_target_updates: 2\n",
      "  iterations_since_restore: 2\n",
      "  node_ip: 127.0.0.1\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 28.474999999999998\n",
      "    ram_util_percent: 90.225\n",
      "  pid: 6613\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.025778026371211796\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.019466007625187312\n",
      "    mean_inference_ms: 0.5433225965166426\n",
      "    mean_raw_obs_processing_ms: 0.0809834791825606\n",
      "  time_since_restore: 4.021952152252197\n",
      "  time_this_iter_s: 3.214308261871338\n",
      "  time_total_s: 4.021952152252197\n",
      "  timers:\n",
      "    learn_throughput: 5210.639\n",
      "    learn_time_ms: 6.141\n",
      "    load_throughput: 202960.423\n",
      "    load_time_ms: 0.158\n",
      "  timestamp: 1644471615\n",
      "  timesteps_since_restore: 64\n",
      "  timesteps_this_iter: 32\n",
      "  timesteps_total: 2000\n",
      "  training_iteration: 2\n",
      "  trial_id: e9055_00000\n",
      "  \n",
      "== Status ==\n",
      "Current time: 2022-02-10 14:40:15 (running for 00:00:05.36)\n",
      "Memory usage on this node: 14.4/16.0 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 0/8 CPUs, 0/0 GPUs, 0.0/8.44 GiB heap, 0.0/4.22 GiB objects\n",
      "Result logdir: /Users/bumjin/ray_results/DQN\n",
      "Number of trials: 1/1 (1 TERMINATED)\n",
      "+--------------------------+------------+----------------+--------+------------------+------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name               | status     | loc            |   iter |   total time (s) |   ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|--------------------------+------------+----------------+--------+------------------+------+----------+----------------------+----------------------+--------------------|\n",
      "| DQN_test_env_e9055_00000 | TERMINATED | 127.0.0.1:6613 |      2 |          4.02195 | 2000 |      100 |                  100 |                  100 |                997 |\n",
      "+--------------------------+------------+----------------+--------+------------------+------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-10 14:40:15,530\tINFO tune.py:636 -- Total run time: 5.63 seconds (5.35 seconds for the tuning loop).\n"
     ]
    }
   ],
   "source": [
    "analysis = ray.tune.run('DQN', config={\"env\":\"test_env\", \"framework\":\"torch\"}, stop={\"training_iteration\":2} )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "728ed223cfa85ea1ef5dcc6c79a939ffd9902707d91f95b40f547e46903ca84f"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
